"""
EIT å’ä¸­é¢„ååˆ†æè„šæœ¬
é’ˆå¯¹3ä¸ªæœˆåŠŸèƒ½ç»“å±€(mRS)çš„é¢„æµ‹åˆ†æ

åŠŸèƒ½ï¼š
1. å¯¹å’ä¸­æ‚£è€…3ä¸ªæœˆæ”¹è‰¯Rankinè¯„åˆ†(mRS)è¿›è¡ŒäºŒåˆ†ç±»é¢„æµ‹
2. é‡‡ç”¨è§£å‰–å­¦å…ˆéªŒçŸ¥è¯†è¿›è¡Œç‰¹å¾åˆç­›ï¼ˆèƒŒä¾§ã€å·¦åä¾§ã€å³åä¾§åŒºåŸŸï¼‰
3. ç»“åˆLogistic Regression L1æ­£åˆ™åŒ–è¿›è¡ŒäºŒæ¬¡ç‰¹å¾é€‰æ‹©
4. ä½¿ç”¨ç•™ä¸€äº¤å‰éªŒè¯(LOOCV)è¯„ä¼°æ¨¡å‹æ€§èƒ½
5. ç”Ÿæˆå®Œæ•´çš„å¯è§†åŒ–åˆ†ææŠ¥å‘Š

æ³¨æ„ï¼šæ­¤ä»£ç å¯¹åº”è®ºæ–‡ä¸­"Exploratory observation of 3-month functional outcome"éƒ¨åˆ†
æ ·æœ¬é‡ï¼šn=13 (mRS 0-2 vs mRS â‰¥3)

ä½œè€…ï¼šæ—æ•¬ç
æ—¥æœŸï¼š2026å¹´1æœˆ
"""

import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
import os
import re
import warnings
from typing import List, Tuple, Optional, Dict

from sklearn.linear_model import LogisticRegression 
from sklearn.model_selection import LeaveOneOut
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectFromModel
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    accuracy_score, roc_auc_score, f1_score, confusion_matrix, 
    roc_curve
)

# ==========================================
# 0. åŸºç¡€é…ç½®
# ==========================================
warnings.filterwarnings('ignore')
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

# ==========================================
# 1. è§£å‰–å­¦æ˜ å°„æ¨¡å—ï¼ˆä¸ä¸»åˆ†æä¸€è‡´ï¼‰
# ==========================================

def get_pair_key(e1: int, e2: int) -> Tuple[int, int]:
    """æ ‡å‡†åŒ–ç”µæå¯¹ï¼Œå¤„ç† (16, 1) é—­ç¯æƒ…å†µ"""
    if (e1 == 16 and e2 == 1) or (e1 == 1 and e2 == 16):
        return (16, 1)
    return tuple(sorted((e1, e2)))

# å®šä¹‰ 16 ä¸ªç›¸é‚»æµ‹é‡å¯¹çš„è§£å‰–å½’å±
ZONE_MAP = {
    get_pair_key(15, 16): "è…¹ä¾§", get_pair_key(16, 1): "è…¹ä¾§", get_pair_key(1, 2): "è…¹ä¾§",
    get_pair_key(2, 3): "å·¦å‰ä¾§", get_pair_key(3, 4): "å·¦å‰ä¾§",
    get_pair_key(4, 5): "å·¦ä¾§",
    get_pair_key(5, 6): "å·¦åä¾§", get_pair_key(6, 7): "å·¦åä¾§",
    get_pair_key(7, 8): "èƒŒä¾§", get_pair_key(8, 9): "èƒŒä¾§", get_pair_key(9, 10): "èƒŒä¾§",
    get_pair_key(10, 11): "å³åä¾§", get_pair_key(11, 12): "å³åä¾§",
    get_pair_key(12, 13): "å³ä¾§",
    get_pair_key(13, 14): "å³å‰ä¾§", get_pair_key(14, 15): "å³å‰ä¾§"
}

def decode_eit_channel(channel_index: int) -> dict:
    """å°† 0-191 é€šé“ç´¢å¼•è§£ç ä¸ºç‰©ç†ç”µæå¯¹å’ŒåŒºåŸŸ"""
    channel_num = channel_index + 1
    frame_idx = (channel_num - 1) // 12
    inj_1 = frame_idx + 1
    inj_2 = ((inj_1 + 8 - 1) % 16) + 1
    valid_pairs = []
    for i in range(1, 17):
        e_a = i
        e_b = (i % 16) + 1
        if not (e_a == inj_1 or e_b == inj_1 or e_a == inj_2 or e_b == inj_2):
            valid_pairs.append(get_pair_key(e_a, e_b))
    meas_idx_in_frame = (channel_num - 1) % 12
    meas_pair = valid_pairs[meas_idx_in_frame] if meas_idx_in_frame < len(valid_pairs) else (0, 0)
    return {"position": ZONE_MAP.get(meas_pair, "æœªçŸ¥")}

def extract_channel_idx(name: str) -> Optional[int]:
    """ä»ç‰¹å¾åæå– channel æ•°å­—"""
    match = re.search(r'channel[_\s]*(\d+)', name.lower())
    return int(match.group(1)) if match else None

# ==========================================
# 2. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
# ==========================================

def standardize_id(id_str):
    """æ ‡å‡†åŒ–æ‚£è€…ID"""
    if pd.isna(id_str): 
        return None
    return os.path.basename(str(id_str).strip()).split('.')[0]

def load_prognosis_data():
    """
    åŠ è½½é¢„ååˆ†ææ•°æ®
    è¿”å›ï¼šç‰¹å¾çŸ©é˜µXï¼Œæ ‡ç­¾yï¼Œç‰¹å¾ååˆ—è¡¨
    æ³¨æ„ï¼šæ­¤å‡½æ•°éœ€è¦å®é™…çš„ä¸´åºŠæ•°æ®æ–‡ä»¶
    """
    # è„±æ•è·¯å¾„ - ä½¿ç”¨é€šç”¨æ–‡ä»¶å
    clin_path = "stroke_prognosis_data.xlsx"
    feat_path = "Wavelet_Feature_Matrix.csv"

    
    if not os.path.exists(clin_path):
        raise FileNotFoundError(f"é¢„åæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {clin_path}")
    if not os.path.exists(feat_path):
        raise FileNotFoundError(f"ç‰¹å¾çŸ©é˜µæ–‡ä»¶ä¸å­˜åœ¨: {feat_path}")
    
    # åŠ è½½æ•°æ®
    df_clin = pd.read_excel(clin_path)
    df_feat = pd.read_csv(feat_path, index_col=0)
    
    # æ ‡å‡†åŒ–ID
    df_clin['ID_Clean'] = df_clin['PatientID'].apply(standardize_id)
    df_feat.index = pd.Series(df_feat.index).apply(standardize_id)
    
    # å¯¹é½æ ·æœ¬
    common = sorted(list(set(df_clin['ID_Clean'].dropna()) & set(df_feat.index.dropna())))
    print(f"âœ… åŒ¹é…æˆåŠŸæ ·æœ¬æ•°: {len(common)}")
    
    # æå–3ä¸ªæœˆmRSè¯„åˆ†
    # å‡è®¾æ•°æ®ä¸­æœ‰'3month_mRS'åˆ—
    df_clin_align = df_clin.set_index('ID_Clean').loc[common]
    df_feat_align = df_feat.loc[common]
    # äºŒåˆ†ç±»ï¼šmRS 0-2 vs mRS â‰¥3
    y_raw = pd.to_numeric(df_clin_align['3ä¸ªæœˆ'], errors='coerce').values
    valid_mask = ~np.isnan(y_raw)
    
    X = df_feat_align[valid_mask].values
    y = np.where(y_raw[valid_mask] >= 3, 1, 0)  # mRS â‰¥3 ä¸ºé˜³æ€§
    
    return X, y, df_feat_align.columns.tolist()

def filter_features_by_anatomy(feature_names: List[str], target_regions: List[str]) -> List[str]:
    """
    åŸºäºè§£å‰–å­¦å…ˆéªŒçŸ¥è¯†ç­›é€‰ç‰¹å¾
    æ ¹æ®è®ºæ–‡å‘ç°ï¼Œå’ä¸­é¢„åç›¸å…³ä¿¡å·é›†ä¸­åœ¨èƒŒä¾§åŒºåŸŸ
    """
    filtered = []
    for f_name in feature_names:
        idx = extract_channel_idx(f_name)
        if idx is None:
            continue
        try:
            pos = decode_eit_channel(idx)['position']
            if pos in target_regions:
                filtered.append(f_name)
        except:
            continue
    return filtered

# ==========================================
# 3. åŒé‡ç­›é€‰LOOCVåˆ†æ
# ==========================================

def run_prognosis_analysis(X, y, feature_names):
    """
    æ‰§è¡Œé¢„ååˆ†ææµç¨‹ï¼š
    1. è§£å‰–å­¦ç­›é€‰ï¼ˆç¬¬ä¸€é˜¶æ®µï¼‰
    2. L1æ­£åˆ™åŒ–ç‰¹å¾é€‰æ‹©ï¼ˆç¬¬äºŒé˜¶æ®µï¼‰
    3. LOOCVè¯„ä¼°
    """
    # ç¬¬ä¸€é˜¶æ®µï¼šè§£å‰–å­¦ç­›é€‰ï¼ˆåŸºäºè®ºæ–‡å‘ç°çš„èƒŒä¾§æ•æ„ŸåŒºåŸŸï¼‰
    target_regions = ['èƒŒä¾§', 'å·¦åä¾§', 'å³åä¾§']  # æ ¹æ®è®ºæ–‡å‘ç°
    selected_features = filter_features_by_anatomy(feature_names, target_regions)
    feature_indices = [i for i, n in enumerate(feature_names) if n in selected_features]
    X_selected = X[:, feature_indices]
    
    print(f"è§£å‰–å­¦ç­›é€‰å®Œæˆ: {len(feature_names)} â†’ {len(selected_features)} ä¸ªç‰¹å¾")
    print(f"ç­›é€‰åŒºåŸŸ: {', '.join(target_regions)}")
    
    # ç¬¬äºŒé˜¶æ®µï¼šLOOCV + L1ç‰¹å¾é€‰æ‹©
    loo = LeaveOneOut()
    y_true, y_pred, y_proba = [], [], []
    feature_selection_counts = {f: 0 for f in selected_features}
    
    # æ„å»ºPipeline
    selector = SelectFromModel(
        LogisticRegression(penalty='l1', solver='liblinear', C=0.5, 
                          class_weight='balanced', random_state=42),
        threshold="mean"
    )
    
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('selector', selector),
        ('classifier', LogisticRegression(solver='liblinear', 
                                         class_weight='balanced', 
                                         random_state=42))
    ])
    
    # LOOCV
    n_samples = len(X_selected)
    print(f"å¼€å§‹LOOCVåˆ†æ (N={n_samples})...")
    
    for fold, (train_idx, test_idx) in enumerate(loo.split(X_selected)):
        X_train, X_test = X_selected[train_idx], X_selected[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]
        
        pipeline.fit(X_train, y_train)
        
        # ç»Ÿè®¡ç‰¹å¾é€‰æ‹©
        support = pipeline.named_steps['selector'].get_support()
        for idx, selected in enumerate(support):
            if selected:
                feature_selection_counts[selected_features[idx]] += 1
        
        # é¢„æµ‹
        y_true.append(y_test[0])
        y_pred.append(pipeline.predict(X_test)[0])
        y_proba.append(pipeline.predict_proba(X_test)[0, 1])
        
        print(f"è¿›åº¦: {fold+1}/{n_samples}", end='\r')
    
    # è®¡ç®—ç‰¹å¾ç¨³å®šæ€§
    feature_stability = pd.DataFrame([
        {'Feature': f, 'Selection_Frequency': count/n_samples}
        for f, count in feature_selection_counts.items()
    ]).sort_values('Selection_Frequency', ascending=False)
    
    return (np.array(y_true), np.array(y_pred), np.array(y_proba), 
            feature_stability, selected_features)

# ==========================================
# 4. å¯è§†åŒ–ä¸æŠ¥å‘Š
# ==========================================

def generate_prognosis_report(y_true, y_pred, y_proba, feature_stability, output_dir):
    """ç”Ÿæˆé¢„ååˆ†ææŠ¥å‘Š"""
    
    metrics = {
        'Accuracy': accuracy_score(y_true, y_pred),
        'AUC': roc_auc_score(y_true, y_proba),
        'F1_Score': f1_score(y_true, y_pred)
    }
    
    # åˆ›å»ºå¯è§†åŒ–
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle('EIT-based Stroke Prognosis Analysis (3-month mRS)', fontsize=16, fontweight='bold')
    
    # 1. ROCæ›²çº¿
    fpr, tpr, _ = roc_curve(y_true, y_proba)
    axes[0, 0].plot(fpr, tpr, color='#2E86AB', lw=3, 
                    label=f'AUC = {metrics["AUC"]:.3f}')
    axes[0, 0].plot([0, 1], [0, 1], 'k--', alpha=0.3)
    axes[0, 0].set_xlabel('False Positive Rate')
    axes[0, 0].set_ylabel('True Positive Rate')
    axes[0, 0].set_title('ROC Curve')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['mRS 0-2', 'mRS â‰¥3'],
                yticklabels=['mRS 0-2', 'mRS â‰¥3'],
                ax=axes[0, 1])
    axes[0, 1].set_title('Confusion Matrix')
    axes[0, 1].set_ylabel('True Label')
    axes[0, 1].set_xlabel('Predicted Label')
    
    # 3. ç‰¹å¾ç¨³å®šæ€§
    top_features = feature_stability.head(10)
    colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(top_features)))
    axes[1, 0].barh(range(len(top_features)), top_features['Selection_Frequency'], color=colors)
    axes[1, 0].set_yticks(range(len(top_features)))
    axes[1, 0].set_yticklabels(top_features['Feature'].str[:30])  # æˆªæ–­é•¿ç‰¹å¾å
    axes[1, 0].set_xlabel('Selection Frequency (LOOCV)')
    axes[1, 0].set_title('Top 10 Stable Features')
    axes[1, 0].invert_yaxis()
    
    # 4. æ€§èƒ½æŒ‡æ ‡
    axes[1, 1].axis('off')
    metrics_text = '\n'.join([f'{k}: {v:.3f}' for k, v in metrics.items()])
    axes[1, 1].text(0.1, 0.5, metrics_text, fontsize=12, 
                    verticalalignment='center',
                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    
    # ä¿å­˜
    report_path = os.path.join(output_dir, 'prognosis_analysis_report.png')
    plt.savefig(report_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    # ä¿å­˜ç»“æœ
    results_df = pd.DataFrame({
        'Metric': list(metrics.keys()),
        'Value': list(metrics.values())
    })
    results_path = os.path.join(output_dir, 'prognosis_results.csv')
    results_df.to_csv(results_path, index=False)
    
    feature_stability_path = os.path.join(output_dir, 'feature_stability.csv')
    feature_stability.to_csv(feature_stability_path, index=False)
    
    return metrics

# ==========================================
# 5. ä¸»å‡½æ•°
# ==========================================

def main():
    """ä¸»åˆ†ææµç¨‹"""
    
    print("=" * 60)
    print("EIT Stroke Prognosis Analysis (3-month functional outcome)")
    print("=" * 60)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = 'EIT_Prognosis_Analysis_Results'
    os.makedirs(output_dir, exist_ok=True)
    
    try:
        # åŠ è½½æ•°æ®
        print("\nğŸ“Š åŠ è½½æ•°æ®...")
        X, y, feature_names = load_prognosis_data()
        
        print(f"æ ·æœ¬é‡: {len(y)}")
        print(f"ç±»åˆ«åˆ†å¸ƒ: {np.bincount(y)} (0: mRS 0-2, 1: mRS â‰¥3)")
        
        # æ‰§è¡Œåˆ†æ
        print("\nğŸ”¬ æ‰§è¡ŒåŒé‡ç­›é€‰é¢„ååˆ†æ...")
        y_true, y_pred, y_proba, feature_stability, selected_features = run_prognosis_analysis(
            X, y, feature_names
        )
        
        # ç”ŸæˆæŠ¥å‘Š
        print("\nğŸ“ˆ ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        metrics = generate_prognosis_report(
            y_true, y_pred, y_proba, feature_stability, output_dir
        )
        
        # æ‰“å°ç»“æœæ‘˜è¦
        print("\n" + "=" * 60)
        print("âœ¨ åˆ†æå®Œæˆï¼")
        print("=" * 60)
        print(f"\nğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
        for metric, value in metrics.items():
            print(f"  {metric}: {value:.3f}")
        
        print(f"\nğŸ“ ç»“æœä¿å­˜è‡³: {output_dir}/")
        print("  - prognosis_analysis_report.png (å¯è§†åŒ–æŠ¥å‘Š)")
        print("  - prognosis_results.csv (æ€§èƒ½æŒ‡æ ‡)")
        print("  - feature_stability.csv (ç‰¹å¾ç¨³å®šæ€§)")
        
        print(f"\nğŸ’¡ æ ¸å¿ƒå‘ç°:")
        print(f"  â€¢ åŸºäºèƒŒä¾§åŒºåŸŸç‰¹å¾å¯é¢„æµ‹3ä¸ªæœˆåŠŸèƒ½ç»“å±€")
        print(f"  â€¢ AUC = {metrics['AUC']:.3f} (n={len(y)})")
        print(f"  â€¢ æœ€ç¨³å®šçš„ç‰¹å¾: {feature_stability.iloc[0]['Feature'][:40]}...")
        
    except Exception as e:
        print(f"\nâŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()