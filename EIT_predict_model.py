"""
EIT æœ€ç»ˆè®­ç»ƒè„šæœ¬ (è¯Šæ–­ä¿®å¤ç‰ˆ v4)
åŠŸèƒ½æ”¹è¿›: 
1. åœ¨æ¯ä¸ªå®éªŒå¾ªç¯å†…éƒ¨å¢åŠ  Try-Catchï¼Œé˜²æ­¢å•ä¸ªå®éªŒæŠ¥é”™å¯¼è‡´ç¨‹åºä¸­æ–­ã€‚
2. å¦‚æœå®éªŒå¤±è´¥ï¼Œç»“æœè¡¨ä¸­ä¼šæ˜¾ç¤ºå…·ä½“çš„é”™è¯¯ä¿¡æ¯ã€‚
3. ä¿æŒä¹‹å‰çš„ SHAP ä¿®å¤å’Œæš´åŠ›æ¸…æ´—é€»è¾‘ã€‚
"""

import pandas as pd
import numpy as np
import os
import re
import warnings
import sys
import traceback
from typing import List, Tuple, Optional, Dict
from collections import Counter

# æœºå™¨å­¦ä¹ åº“
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score
from sklearn.feature_selection import SelectKBest, f_classif
from xgboost import XGBClassifier

# å°è¯•å¯¼å…¥ SMOTE
try:
    from imblearn.over_sampling import SMOTE
except ImportError:
    print("âš ï¸ è­¦å‘Š: æœªå®‰è£… imbalanced-learnï¼Œå°†è·³è¿‡ SMOTEã€‚")
    SMOTE = None

# å°è¯•å¯¼å…¥ SHAP
try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    print("âš ï¸ è­¦å‘Š: æœªå®‰è£… shapï¼Œå°†è·³è¿‡ SHAP åˆ†æã€‚")
    SHAP_AVAILABLE = False

warnings.filterwarnings('ignore')

# ==========================================
# --- 1. é…ç½®åŒºåŸŸ ---
# ==========================================

CLINICAL_PATH = r"clinical_data.xlsx"
FEATURE_PATH = r"Wavelet_Feature_Matrix.csv"

OUTPUT_DIR = 'EIT_Final_Training_Result_MultiExp_Fixed_v4'
os.makedirs(OUTPUT_DIR, exist_ok=True)

RANDOM_STATE = 42
K_FEATURES = 100 
SHAP_TOP_N = 50 

FOCUSED_TARGETS = ['å·¦å˜å¼‚åº¦', 'å³å˜å¼‚åº¦(%)','è„‘æ¢—æ­»é¢ç§¯','å‘ç—…å¤©æ•°','æ°§åˆæŒ‡æ•°','è‚¾å°çƒæ»¤è¿‡ç‡ï¼ˆEPI-cys)']

CUSTOM_LOCATION_FILTER_BY_TARGET = {
    'å·¦å˜å¼‚åº¦': ['èƒŒä¾§', 'å³å‰ä¾§', 'å·¦åä¾§'],
    'å³å˜å¼‚åº¦(%)': ['èƒŒä¾§', 'å³å‰ä¾§', 'å·¦åä¾§'],
    'è„‘æ¢—æ­»é¢ç§¯': ['èƒŒä¾§', 'å·¦åä¾§', 'å³åä¾§'],
    'å‘ç—…å¤©æ•°': ['èƒŒä¾§', 'å·¦åä¾§', 'å³åä¾§'],
    'æ°§åˆæŒ‡æ•°': ['èƒŒä¾§', 'å·¦åä¾§', 'å³åä¾§'],
    'è‚¾å°çƒæ»¤è¿‡ç‡ï¼ˆEPI-cys)': ['èƒŒä¾§', 'å·¦åä¾§', 'å³åä¾§']
}

PARAM_GRID_XGBC = {
    'n_estimators': [100, 150],
    'max_depth': [3, 5],
    'learning_rate': [0.1],
    'subsample': [0.8]
}

# ==========================================
# --- 2. è¾…åŠ©å‡½æ•° ---
# ==========================================

def standardize_id(id_str):
    if pd.isna(id_str): return None
    key = str(id_str).strip()
    key = os.path.basename(key) 
    if key.lower().endswith(('.xlsx', '.csv')): key = key.rsplit('.', 1)[0]
    return key.strip()

def get_pair_key(e1: int, e2: int) -> Tuple[int, int]:
    if (e1 == 16 and e2 == 1) or (e1 == 1 and e2 == 16): return (16, 1)
    return tuple(sorted((e1, e2)))

ZONE_MAP = {
    get_pair_key(15, 16): "è…¹ä¾§", get_pair_key(16, 1): "è…¹ä¾§", get_pair_key(1, 2): "è…¹ä¾§",
    get_pair_key(2, 3): "å·¦å‰ä¾§", get_pair_key(3, 4): "å·¦å‰ä¾§",
    get_pair_key(4, 5): "å·¦ä¾§",
    get_pair_key(5, 6): "å·¦åä¾§", get_pair_key(6, 7): "å·¦åä¾§",
    get_pair_key(7, 8): "èƒŒä¾§", get_pair_key(8, 9): "èƒŒä¾§", get_pair_key(9, 10): "èƒŒä¾§",
    get_pair_key(10, 11): "å³åä¾§", get_pair_key(11, 12): "å³åä¾§",
    get_pair_key(12, 13): "å³ä¾§",
    get_pair_key(13, 14): "å³å‰ä¾§", get_pair_key(14, 15): "å³å‰ä¾§"
}

def decode_eit_channel(channel_index: int) -> dict:
    channel_num = channel_index + 1
    frame_idx = (channel_num - 1) // 12
    inj_1 = frame_idx + 1
    inj_2 = ((inj_1 + 8 - 1) % 16) + 1
    valid_pairs = []
    for i in range(1, 17):
        e_a = i
        e_b = (i % 16) + 1
        if not (e_a == inj_1 or e_b == inj_1 or e_a == inj_2 or e_b == inj_2):
            valid_pairs.append(get_pair_key(e_a, e_b))
    meas_idx_in_frame = (channel_num - 1) % 12
    meas_pair = valid_pairs[meas_idx_in_frame] if meas_idx_in_frame < len(valid_pairs) else (0, 0)
    return {"position": ZONE_MAP.get(meas_pair, "æœªçŸ¥")}

def extract_channel_idx(name: str) -> Optional[int]:
    match = re.search(r'channel[_\s]*(\d+)', name.lower())
    return int(match.group(1)) if match else None

def filter_features_by_custom_locations(feature_names: List[str], target_positions: List[str], mode: str = 'include') -> List[str]:
    if not target_positions: return feature_names
    filtered = []
    for f_name in feature_names:
        idx = extract_channel_idx(f_name)
        if idx is None:
            filtered.append(f_name) 
            continue
        try:
            pos = decode_eit_channel(idx)['position']
            is_in_target = pos in target_positions
            if mode == 'include':
                if is_in_target: filtered.append(f_name)
            elif mode == 'exclude':
                if not is_in_target: filtered.append(f_name)
        except: 
            filtered.append(f_name)
    return filtered

def get_cutoff(series):
    name = series.name
    if name == 'å‘ç—…å¤©æ•°': return lambda x: 1 if x >= 14 else 0
    if name == 'è„‘æ¢—æ­»é¢ç§¯': return lambda x: 1 if x >= 4.5 else 0
    if name in ['å·¦å˜å¼‚åº¦', 'å³å˜å¼‚åº¦(%)']: return lambda x: 1 if x < 20 else 0
    if 'æ»¤è¿‡ç‡' in name: return lambda x: 1 if x < 90 else 0
    if 'æ°§åˆ' in name: return lambda x: 1 if x < 300 else 0
    return lambda x: 1 if x >= series.median() else 0

def analyze_shap_distribution(model, X_test: pd.DataFrame, top_n: int = 50) -> str:
    feature_importance = None
    method_used = "None"

    # 1. å°è¯•ä½¿ç”¨ SHAP (å¼ºåˆ¶ Numpy æ¨¡å¼)
    if SHAP_AVAILABLE:
        try:
            explainer = shap.TreeExplainer(model)
            # å…³é”®ä¿®å¤ï¼šåªä¼  valuesï¼Œåˆ‡æ–­ pandas metadata çš„å¹²æ‰°
            X_numpy = X_test.values.astype(np.float64)
            
            # check_additivity=False å¯ä»¥é˜²æ­¢å› ç²¾åº¦é—®é¢˜å¯¼è‡´çš„æŠ¥é”™
            shap_values = explainer.shap_values(X_numpy, check_additivity=False)
            
            if isinstance(shap_values, list):
                vals = shap_values[1]
            else:
                vals = shap_values

            feature_importance = np.abs(vals).mean(axis=0)
            method_used = "SHAP"
        except Exception as e:
            print(f"   âš ï¸ SHAP Numpy æ¨¡å¼ä¾ç„¶æŠ¥é”™: {e}")
            print("   âš ï¸ æ­£åœ¨åˆ‡æ¢åˆ° XGBoost åŸç”Ÿ Feature Importance (Gain)...")
    
    # 2. å¦‚æœ SHAP å¤±è´¥ï¼Œé™çº§ä½¿ç”¨ XGBoost è‡ªå¸¦çš„é‡è¦æ€§
    if feature_importance is None:
        try:
            feature_importance = model.feature_importances_
            method_used = "XGB_Gain"
        except Exception as e:
             print(f"   âŒ è¿ XGBoost åŸç”Ÿé‡è¦æ€§éƒ½è·å–å¤±è´¥: {e}")
             return "Analysis Failed"

    # 3. ç»Ÿè®¡åŒºåŸŸåˆ†å¸ƒ
    try:
        feat_imp_df = pd.DataFrame({
            'Feature': X_test.columns,
            'Importance': feature_importance
        }).sort_values(by='Importance', ascending=False)
        
        top_features = feat_imp_df.head(top_n)['Feature'].tolist()
        
        zone_counter = Counter()
        for f_name in top_features:
            idx = extract_channel_idx(f_name)
            if idx is not None:
                info = decode_eit_channel(idx)
                zone_counter[info['position']] += 1
            else:
                zone_counter['éChannelç‰¹å¾'] += 1
                
        sorted_zones = zone_counter.most_common()
        result_str = " > ".join([f"{zone}({count})" for zone, count in sorted_zones])
        
        return f"[{method_used}] {result_str}" if result_str else f"[{method_used}] æ— æ˜¾è‘—ç‰¹å¾"
        
    except Exception as e:
        print(f"   âŒ ç»“æœæ±‡æ€»é˜¶æ®µå‡ºé”™: {e}")
        return "Summary Error"

# ==========================================
# --- 3. æ ¸å¿ƒè®­ç»ƒæµç¨‹ ---
# ==========================================

def aggressive_clean(df):
    # 1. æ¸…æ´—åˆ—å
    df.columns = df.columns.str.replace(r'[\[\]\'\"]', '', regex=True)
    # 2. æ¸…æ´—æ•°æ®å†…å®¹
    obj_cols = df.select_dtypes(include=['object']).columns
    if len(obj_cols) > 0:
        for col in obj_cols:
            df[col] = df[col].astype(str).str.replace(r'[\[\]\'\"]', '', regex=True)
            df[col] = pd.to_numeric(df[col], errors='coerce')
    # 3. å†æ¬¡ç¡®ä¿æ‰€æœ‰åˆ—éƒ½æ˜¯ float (åŒé‡ä¿é™©)
    df = df.astype(float)
    print("   ğŸ§¹ æ•°æ®æ¸…æ´—å®Œæˆï¼Œæ‰€æœ‰åˆ—å·²è½¬æ¢ä¸ºæ•°å€¼ã€‚")
    return df

def load_and_align():
    if not os.path.exists(CLINICAL_PATH): raise FileNotFoundError(f"æœªæ‰¾åˆ°ä¸´åºŠæ•°æ®: {CLINICAL_PATH}")
    if not os.path.exists(FEATURE_PATH): raise FileNotFoundError(f"æœªæ‰¾åˆ°ç‰¹å¾çŸ©é˜µ: {FEATURE_PATH}")

    print(f"ğŸ“Š åŠ è½½ä¸´åºŠæ•°æ®: {os.path.basename(CLINICAL_PATH)}")
    df_c = pd.read_excel(CLINICAL_PATH) if CLINICAL_PATH.endswith('.xlsx') else pd.read_csv(CLINICAL_PATH)
    print(f"ğŸ“Š åŠ è½½ç‰¹å¾çŸ©é˜µ: {os.path.basename(FEATURE_PATH)}")
    df_f = pd.read_csv(FEATURE_PATH, index_col=0)

    # æš´åŠ›æ¸…æ´—
    df_f = aggressive_clean(df_f)

    # å¯¹é½
    id_col = next((c for c in df_c.columns if 'ID' in c.upper()), df_c.columns[0])
    df_c['PID_Clean'] = df_c[id_col].apply(standardize_id)
    df_f.index = pd.Series(df_f.index).apply(standardize_id)
    
    common = sorted(list(set(df_c['PID_Clean'].dropna()) & set(df_f.index.dropna())))
    
    df_c_align = df_c.set_index('PID_Clean').loc[common]
    df_f_align = df_f.loc[common]
    return df_c_align, df_f_align


def main():
    try:
        df_clinical, df_features = load_and_align()
        results = []
        
        print("\nğŸš€ å¼€å§‹å¤šæ¨¡å¼è®­ç»ƒæµç¨‹ (è¯Šæ–­ç‰ˆ)...")
        
        for target in FOCUSED_TARGETS:
            if target not in df_clinical.columns:
                print(f"âš ï¸ è­¦å‘Š: ä¸´åºŠæ•°æ®ä¸­ç¼ºå°‘ç›®æ ‡åˆ— '{target}'")
                continue
                
            print(f"\n{'='*30}\nğŸ¯ Target: {target}\n{'='*30}")
            
            y_raw = df_clinical[target].dropna()
            if len(y_raw) < 20: continue
                
            cutoff_fn = get_cutoff(y_raw)
            y_cls = y_raw.apply(cutoff_fn)
            if len(y_cls.unique()) < 2: continue
            
            X_base = df_features.loc[y_raw.index].copy()
            target_roi_list = CUSTOM_LOCATION_FILTER_BY_TARGET.get(target, [])
            
            experiments = [
                ('1_All_Locations', None, 'include'),
                ('2_ROI_Only', target_roi_list, 'include'),
                ('3_ROI_Removed', target_roi_list, 'exclude')
            ]
            
            for exp_name, roi_list, mode in experiments:
                print(f"\n--- [å®éªŒ: {exp_name}] ---")
                
                # --- å°†å•ä¸ªå®éªŒåŒ…è£¹åœ¨ try-catch ä¸­ ---
                try:
                    # ç‰¹å¾ç­›é€‰
                    if exp_name == '1_All_Locations':
                        X_loc = X_base
                    else:
                        cols = filter_features_by_custom_locations(X_base.columns.tolist(), roi_list, mode=mode)
                        X_loc = X_base[cols]
                    
                    if X_loc.shape[1] == 0: 
                        print("âš ï¸ ç‰¹å¾æ•°é‡ä¸º0ï¼Œè·³è¿‡")
                        continue

                    # æ•°æ®åˆ’åˆ†
                    X_train, X_test, y_train, y_test = train_test_split(
                        X_loc, y_cls, test_size=0.2, random_state=RANDOM_STATE, stratify=y_cls
                    )
                    
                    # é¢„å¤„ç†
                    imputer = SimpleImputer(strategy='median')
                    X_train_imp = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)
                    X_test_imp = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)
                    
                    # ========== ä¿®å¤SMOTEé¡ºåºï¼ˆå¯é€‰ï¼‰ ==========
                    # å…ˆæ ‡å‡†åŒ–çœŸå®æ•°æ®
                    scaler = StandardScaler()
                    X_train_scaled_raw = pd.DataFrame(scaler.fit_transform(X_train_imp), columns=X_train.columns)
                    X_test_scaled = pd.DataFrame(scaler.transform(X_test_imp), columns=X_test.columns)
                    
                    # å†SMOTEï¼ˆåœ¨æ ‡å‡†åŒ–åçš„æ•°æ®ä¸Šï¼‰
                    if SMOTE:
                        k = min(5, y_train.value_counts().min() - 1) if y_train.value_counts().min() > 1 else 1
                        smote = SMOTE(random_state=RANDOM_STATE, k_neighbors=k)
                        X_train_scaled, y_train_res = smote.fit_resample(X_train_scaled_raw, y_train)
                    else:
                        X_train_scaled, y_train_res = X_train_scaled_raw, y_train
                    # ===========================================
                    
                    # ç‰¹å¾é€‰æ‹©
                    k_best = min(K_FEATURES, X_train_scaled.shape[1])
                    selector = SelectKBest(score_func=f_classif, k=k_best)
                    X_train_selected = selector.fit_transform(X_train_scaled, y_train_res)  # æ”¹ä¸ºX_train_selected
                    X_test_selected = selector.transform(X_test_scaled)                     # æ”¹ä¸ºX_test_selected

                    # è·å–é€‰ä¸­çš„åŸå§‹ç‰¹å¾åï¼ˆç”¨äºSHAPåˆ†æï¼‰
                    selected_feature_names = X_train_scaled.columns[selector.get_support()].tolist()
                    
                    # è®­ç»ƒ
                    grid = GridSearchCV(
                        XGBClassifier(random_state=RANDOM_STATE, scale_pos_weight=1.0, eval_metric='logloss'),
                        param_grid=PARAM_GRID_XGBC, scoring='accuracy', cv=3, n_jobs=-1
                    )
                    grid.fit(X_train_selected, y_train_res)
                    best_model = grid.best_estimator_
                    
                    # é¢„æµ‹
                    y_pred = best_model.predict(X_test_selected)
                    y_proba = best_model.predict_proba(X_test_selected)[:, 1] if hasattr(best_model, "predict_proba") else [0.5]*len(y_test)
                    
                    # ========== ä¿®å¤SHAPåˆ†æ ==========
                    print("   ğŸ” æ­£åœ¨è®¡ç®—ç‰¹å¾ç©ºé—´åˆ†å¸ƒ...")
                    
                    # åˆ›å»ºå¸¦åŸå§‹ç‰¹å¾åçš„DataFrameç”¨äºSHAP
                    if len(selected_feature_names) > 0:
                        X_test_for_shap = pd.DataFrame(
                            X_test_selected,
                            columns=selected_feature_names,
                            index=X_test.index
                        )
                        shap_zone_str = analyze_shap_distribution(best_model, X_test_for_shap, top_n=SHAP_TOP_N)
                    else:
                        shap_zone_str = "æ— é€‰ä¸­ç‰¹å¾"
                    # ===================================
                    
                    # è·å–æœ€é‡è¦çš„ç‰¹å¾
                    top1_idx = np.argmax(selector.scores_) if selector.scores_.size > 0 else 0
                    top1_feature = X_train_scaled.columns[top1_idx] if top1_idx < len(X_train_scaled.columns) else "Unknown"
                    
                    res_dict = {
                        'Target': target,
                        'Experiment': exp_name,
                        'Acc': accuracy_score(y_test, y_pred),
                        'AUC': roc_auc_score(y_test, y_proba) if len(np.unique(y_test))>1 else 0.5,
                        'F1': f1_score(y_test, y_pred, average='macro'),
                        'Top_SHAP_Zones': shap_zone_str,
                        'Feature_Count': X_loc.shape[1],
                        'Top1_Feature': top1_feature
                    }
                    results.append(res_dict)
                    print(f"   ğŸ“Š ç»“æœ: Acc={res_dict['Acc']:.4f}, AUC={res_dict['AUC']:.4f}, Top Zones: {shap_zone_str}")

                except Exception as e:
                    print(f"   âŒ å®éªŒå¤±è´¥: {e}")
                    traceback.print_exc()
                    results.append({
                        'Target': target,
                        'Experiment': exp_name,
                        'Acc': 0, 'AUC': 0, 'F1': 0,
                        'Top_SHAP_Zones': f"FAILED: {str(e)[:50]}...",
                        'Feature_Count': X_loc.shape[1] if 'X_loc' in locals() else 0,
                        'Top1_Feature': 'Error'
                    })
        if results:
            df_res = pd.DataFrame(results)
            # é‡æ–°æ’åˆ—åˆ—é¡ºåº
            cols = ['Target', 'Experiment', 'Acc', 'AUC', 'F1', 'Top_SHAP_Zones', 'Feature_Count', 'Top1_Feature']
            df_res = df_res[cols]
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            save_path = os.path.join(OUTPUT_DIR, 'EIT_Final_Results.xlsx')
            df_res.to_excel(save_path, index=False)
            
            # åŒæ—¶ä¿å­˜ä¸ºCSVï¼ˆå¯è¯»æ€§æ›´å¥½ï¼‰
            csv_path = os.path.join(OUTPUT_DIR, 'EIT_Final_Results.csv')
            df_res.to_csv(csv_path, index=False, encoding='utf-8-sig')
            
            print(f"\n{'='*60}")
            print(f"âœ¨ å®éªŒå®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³:")
            print(f"   Excel: {save_path}")
            print(f"   CSV:   {csv_path}")
            print(f"{'='*60}")
            
            # æ‰“å°æ±‡æ€»è¡¨æ ¼
            print("\nğŸ“Š ç»“æœæ±‡æ€»:")
            print(df_res.to_markdown(index=False))
        else:
            print("âš ï¸ æ²¡æœ‰ç”Ÿæˆä»»ä½•ç»“æœ")
    except Exception as e:
        print(f"âŒ ç¨‹åºä¸»æµç¨‹å´©æºƒ: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    main()